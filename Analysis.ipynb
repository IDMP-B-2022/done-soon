{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done Soon? Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QA2OwOc1X3Fw"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QtKgh08BzA8s"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, RocCurveDisplay\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import f1_score\n",
    "import pymongo\n",
    "import plotly.express as px\n",
    "import math\n",
    "from enum import Enum\n",
    "from tqdm.notebook import tqdm\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstanceType(Enum):\n",
    "    ALL = 1\n",
    "    OPTIMIZATION = 2\n",
    "    SAT = 3\n",
    "\n",
    "data_types_to_consider = InstanceType.OPTIMIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data - Mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_instance_type(s):\n",
    "    return InstanceType.SAT if s == 'SAT' else InstanceType.OPTIMIZATION\n",
    "\n",
    "\n",
    "conn_str = f\"mongodb://localhost\"\n",
    "mongo_client = pymongo.MongoClient(conn_str, port=27017)\n",
    "\n",
    "#all data\n",
    "all_data = pd.DataFrame(mongo_client.done_soon.problems.find({}))\n",
    "all_data = all_data[all_data['error'] !=True ]\n",
    "all_data = all_data[all_data['problem_type'].map(string_to_instance_type) == data_types_to_consider ]\n",
    "\n",
    "#manual filtering (2hour)\n",
    "timeout_after_hour = all_data[all_data.time_to_solution > 7199 * 1000] #>2hours\n",
    "finishes_after_lower_bound = all_data[all_data.time_to_solution > 72 * 1000] #>72seconds\n",
    "\n",
    "#print result filtering / Sanity check\n",
    "print(f\"Unsolved: {len(timeout_after_hour)}\")\n",
    "print(f\"Class distribution: {len(timeout_after_hour)/(len(timeout_after_hour)+len(finishes_after_lower_bound))}\")\n",
    "\n",
    "#number of values of len of stats\n",
    "finishes_after_lower_bound.statistics.map(len).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_of_stats = np.array([len(x) for x in all_data['statistics']])\n",
    "time_to_solution = all_data['time_to_solution']\n",
    "plt.scatter(length_of_stats, time_to_solution, marker='.')\n",
    "plt.title(\"Suspicious data (shouldn't this be linear?)\")\n",
    "plt.xlabel(\"length of stats array\")\n",
    "plt.ylabel(\"time to solution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "length_of_stats = np.array([len(x) for x in all_data['statistics']]) / 2\n",
    "time_to_solution = all_data['time_to_solution'] / 72000\n",
    "filtered_all_data = all_data[((all_data['time_to_solution'] / 72000) - length_of_stats) < 5]\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,6)\n",
    "filtered_length_of_stats = np.array([len(x) for x in filtered_all_data['statistics']]) / 2\n",
    "filtered_time_to_solution = filtered_all_data['time_to_solution'] / 72000\n",
    "\n",
    "plt.scatter(length_of_stats, time_to_solution, marker='.', color='red', alpha=0.4)\n",
    "plt.scatter(filtered_length_of_stats, filtered_time_to_solution, marker='.')\n",
    "plt.plot(np.arange(0, 100, 1), np.arange(0, 100, 1), color='black', alpha=0.4)\n",
    "plt.xlabel(\"length of stats array\")\n",
    "plt.ylabel(\"time to solution\")\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_small_data = all_data[all_data['statistics'].map(len) > 5]\n",
    "length_of_stats = np.array([len(x) for x in no_small_data['statistics']]) / 2\n",
    "i = range(0, len(no_small_data.index))\n",
    "plt.barh(i, np.sort(length_of_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data = filtered_all_data\n",
    "filtered_features_at_percent = {}\n",
    "\n",
    "for i in tqdm(range(1,200)):\n",
    "    df_i=[]\n",
    "    for id, problem in filtered_all_data.iterrows(): \n",
    "        for index, p in enumerate(problem.statistics):\n",
    "            if index == i:\n",
    "                new_p = dict(p['features'])\n",
    "                new_p=cleanup(new_p)\n",
    "                if i!=1:\n",
    "                    new_p=gradients(df_prev.loc[id], new_p)\n",
    "                new_p['mzn'] = problem['mzn']\n",
    "                new_p['dzn'] = problem['dzn']\n",
    "                new_p['solved_within_time_limit'] = problem['time_to_solution'] < 7199 * 1000\n",
    "                df_i.append((id, new_p))\n",
    "    df_i = pd.DataFrame([a[1] for a in df_i], index=[a[0] for a in df_i])\n",
    "    df_i=df_i.fillna(value = 0)\n",
    "    if i!=0:   \n",
    "        filtered_features_at_percent[i]=df_i\n",
    "    df_prev=df_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_all_data[filtered_all_data['time_to_solution'] > 3600 * 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_features_at_percent[1]['solved_within_time_limit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some dataframe df\n",
    "all_data['time_to_solution'].sort_values().plot(use_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sub_data=all_data[all_data['time_to_solution']>7200]\n",
    "sub_data['time_to_solution'].sort_values().plot(use_index=False)\n",
    "#plt.plot(all_data[all_data['time_to_solution']>6000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "#finishes_after_lower_bound.statistics[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(df):\n",
    "    del df[\"decision_level_sat\"]\n",
    "    del df[\"ewma_decision_level_mip\"]\n",
    "    del df[\"decision_level_mip\"]\n",
    "#     del df[\"best_objective\"]\n",
    "#     df[\"unassnVar\"]   = (2**df['vars']) - df['opennodes']\n",
    "#     df[\"fracFailUnassn\"]     = df['conflicts'] / df['unassnVar']         # num failures/ num open nodes\n",
    "    df[\"fracOpenVisit\"]  = (df['vars'] - df['opennodes']) / df['opennodes']       # ratio of open nodes to visited nodes (how much of soln space explored)\n",
    "    df[\"fracBoolVars\"]     = df['boolVars'] / df['vars']                 # num bools / total num of vars\n",
    "    df[\"fracPropVars\"]     = df['propagations'] / df['vars']        # num propagations/ total num of vars\n",
    "#     df[\"frac_unassigned\"] = df['unassnVar'] / df['vars']  # current assignments/ total vars\n",
    "    df[\"fracLongClauses\"] = df['long'] + df['bin'] + df['tern']         # fraction of learnt clauses that have more than 3 literals\n",
    "    df[\"freqBackjumps\"]  = df['back_jumps']/df['search_time']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients(df_prev, df_curr):\n",
    "    keys=['conflicts','ewma_conflicts','decisions','search_iterations','opennodes','ewma_opennodes',\n",
    "          'vars','back_jumps','ewma_back_jumps','solutions','total_time','search_time','intVars',\n",
    "          'propagations','sat_propagations','ewma_propagations','propagators','boolVars','learnt',\n",
    "          'bin','tern','long','peak_depth','decision_level_engine','ewma_decision_level_engine',\n",
    "          'decision_level_treesize','clause_mem','prop_mem','ewma_best_objective',\n",
    "          'fracOpenVisit','fracBoolVars','fracPropVars','freqBackjumps', 'best_objective']\n",
    "    for i in keys:\n",
    "        df_curr[i+'_gradient']=(df_curr[i]-df_prev[i])/0.05*7200\n",
    "    return df_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_data[all_data['time_to_solution'] > 7199 * 1000]), len(all_data[all_data['time_to_solution'] < 7199 * 1000]), len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_at_percent = {}\n",
    "\n",
    "for i in tqdm(range(1,200)):\n",
    "    df_i=[]\n",
    "    for id, problem in all_data.iterrows(): \n",
    "        for index, p in enumerate(problem.statistics):\n",
    "            if index == i:\n",
    "                new_p = dict(p['features'])\n",
    "                new_p=cleanup(new_p)\n",
    "                if i!=1:\n",
    "                    new_p=gradients(df_prev.loc[id], new_p)\n",
    "                new_p['mzn'] = problem['mzn']\n",
    "                new_p['dzn'] = problem['dzn']\n",
    "                new_p['solved_within_time_limit'] = problem['time_to_solution'] < 7199 * 1000 \\\n",
    "                or np.logical_not(np.isnan(problem['time_to_solution']))\n",
    "                df_i.append((id, new_p))\n",
    "    df_i = pd.DataFrame([a[1] for a in df_i], index=[a[0] for a in df_i])\n",
    "    df_i=df_i.fillna(value = 0)\n",
    "    if i!=0:   \n",
    "        features_at_percent[i]=df_i\n",
    "    df_prev=df_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans=features_at_percent[20].isna().sum().to_numpy().nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_at_percent[20].keys()[nans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_at_percent[20]\n",
    "print(len(features_at_percent[100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(features_at_percent[10]['ewma_best_objective'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max([len(features_at_percent[199][features_at_percent[199]['solved_within_time_limit'] == False]) for i in range(1, 199)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMoP4AQWalT7"
   },
   "source": [
    "# Data Analysis\n",
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    df1=df.drop(['mzn','dzn'], axis=1)\n",
    "    df1.drop(df1.columns[df1.nunique() == 1], axis=1, inplace=True) #drop cols with constant value\n",
    "    #rescale data\n",
    "    transformer = MaxAbsScaler().fit(df1)\n",
    "    df1 = pd.DataFrame(transformer.transform(df1), columns=df1.columns, index=df1.index) #normalise data\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_split(df, test_size=0.25, random_state=22):\n",
    "    return train_test_split(df.drop(columns = [\"solved_within_time_limit\"]),\\\n",
    "                                df[\"solved_within_time_limit\"], test_size=0.25, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9hPn8jzm0U8c"
   },
   "outputs": [],
   "source": [
    "df=features_at_percent[5] #THE NUMBER HERE IS THE % OF TL\n",
    "df=preprocessing(df)\n",
    "# training-testing split\n",
    "X_train, X_test, y_train, y_test  = train_test_split(df.drop(columns = [\"solved_within_time_limit\"]),\\\n",
    "                                                     df[\"solved_within_time_limit\"], test_size=0.25, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IN-6LHsxsk1w",
    "outputId": "0a5afd18-e7fc-442c-eaa6-5fba63e0cd15"
   },
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6JaEQjH9sk1x",
    "outputId": "01f7b2d6-a07e-46fe-e868-ed9b02b23e4e"
   },
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUDr-kP-Yf_s"
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VcuEaofpzqWg"
   },
   "outputs": [],
   "source": [
    "models = {}\n",
    "\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "models['LR'] = LogisticRegression(max_iter=1000, C=1000 , class_weight = 'balanced',random_state=22)\n",
    "\n",
    "#Support Vector Machine\n",
    "from sklearn.svm import SVC\n",
    "models['SVM'] = SVC(kernel = 'rbf', class_weight = 'balanced', probability = True, random_state=22)\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "models['RF'] = RandomForestClassifier(min_samples_leaf = 5, class_weight = 'balanced_subsample',random_state=22)\n",
    "\n",
    "#Extra Tree\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "models['ET'] = ExtraTreesClassifier(class_weight = 'balanced', random_state=22)\n",
    "\n",
    "#Multi-layered perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "models['MLP'] = MLPClassifier(random_state=22)\n",
    "\n",
    "# Naive Bayes\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "#models['NB'] = GaussianNB()\n",
    "\n",
    "# Adaboost\n",
    "#from sklearn.ensemble import AdaBoostClassifier\n",
    "#models['AB'] = AdaBoostClassifier()\n",
    "\n",
    "#KNN\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "#models['KNN'] = KNeighborsClassifier(weights = 'distance')\n",
    "\n",
    "# Decision Trees\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "models['DT'] = DecisionTreeClassifier(max_depth = 5, class_weight = 'balanced', random_state=22)\n",
    "\n",
    "#Dummy classifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "models['DUM'] = DummyClassifier(strategy=\"most_frequent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicitions and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rf_reg = RandomForestClassifier(min_samples_leaf = 5)\n",
    "rf_reg.fit(X_train,y_train)\n",
    "\n",
    "predictions = rf_reg.predict(X_test)\n",
    "importances = rf_reg.feature_importances_\n",
    "\n",
    "print('The F1 score with all features of a RandomForestClassifier with min_samples_leaf of 5 is ', f1_score(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sorted_importance_indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.title('Feature Importance of Random Forest Regressor')\n",
    "plt.bar(range(len(sorted_importance_indices)), importances[sorted_importance_indices], align='center')\n",
    "plt.xticks(range(len(sorted_importance_indices)), X_train.columns[sorted_importance_indices], rotation=90)\n",
    "plt.rcParams[\"figure.figsize\"] = (20,3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 8 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sorted_importance_indices = np.argsort(importances)[::-1][:8]\n",
    "\n",
    "plt.title('Feature Importance of Random Forest Regressor')\n",
    "plt.bar(range(8), importances[sorted_importance_indices], align='center')\n",
    "plt.xticks(range(8), X_train.columns[sorted_importance_indices], rotation=90)\n",
    "plt.rcParams[\"figure.figsize\"] = (20,3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_importance_indices = np.argsort(importances)[::-1][:15]\n",
    "\n",
    "def f1_with_n_top_features(n, importance_column_indices):\n",
    "    summed = 0\n",
    "    for i in range(0, 10):\n",
    "        X_train, X_test, y_train, y_test  = train_test_split(df.drop(columns = [\"solved_within_time_limit\"]),\\\n",
    "                                                     df[\"solved_within_time_limit\"], test_size=0.25)\n",
    "        rf_reg = RandomForestClassifier(min_samples_leaf = 5)\n",
    "        rf_reg.fit(X_train.iloc[:, importance_column_indices[:n]], y_train)\n",
    "\n",
    "        predictions = rf_reg.predict(X_test.iloc[:, importance_column_indices[:n]])\n",
    "        summed += f1_score(y_test, predictions)\n",
    "    return summed / 10\n",
    "\n",
    "\n",
    "\n",
    "f1_scores = [(i, f1_with_n_top_features(i, sorted_importance_indices)) for i in range(1, len(sorted_importance_indices) + 1)[::-1]]\n",
    "\n",
    "plt.title(f'F1 score for top N features ({\"OPTIMIZATION\" if data_types_to_consider is InstanceType.OPTIMIZATION else \"SAT\"})')\n",
    "plt.plot([a[0] for a in f1_scores], [a[1] for a in f1_scores])\n",
    "plt.gca().set_ylim([0,1])\n",
    "plt.rcParams[\"figure.figsize\"] = (20,3)\n",
    "plt.show()\n",
    "\n",
    "print(f1_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_plt(X_train, y_train, X_test, y_test, perc_TL):\n",
    "    train_accuracy, accuracy, precision, recall, auc, f1 = {}, {}, {}, {}, {}, {}\n",
    "    figs, axs = plt.subplots(2,3,figsize=(20, 12))\n",
    "\n",
    "    for i, key in enumerate(models.keys()):\n",
    "\n",
    "        # Fit the classifier model\n",
    "        models[key].fit(X_train, y_train)\n",
    "\n",
    "        predictions = models[key].predict(X_test)\n",
    "        predictions_prob = models[key].predict_proba(X_test)[:,1]\n",
    "        train_predictions = models[key].predict(X_train)\n",
    "        # Predic  \n",
    "        # Calculate Accuracy, Precision and Recall Metrics\n",
    "        accuracy[key] = accuracy_score(predictions, y_test)\n",
    "        precision[key] = precision_score(predictions, y_test, zero_division=1)\n",
    "        recall[key] = recall_score(predictions, y_test, zero_division=1)\n",
    "        auc[key] = roc_auc_score(y_test, predictions_prob)\n",
    "        train_accuracy[key] = accuracy_score(train_predictions, y_train)\n",
    "        f1[key] = f1_score(y_test,predictions)\n",
    "        #should it be (true, pred)? yes\n",
    "\n",
    "        #To Display\n",
    "        RocCurveDisplay.from_predictions(y_test, predictions_prob, name=key , ax=axs[0,0])\n",
    "        axs[0,1].bar(key, accuracy[key]) \n",
    "        axs[0,2].bar(key, train_accuracy[key]) \n",
    "        axs[1,0].bar(key, recall[key])\n",
    "        axs[1,1].bar(key, precision[key])\n",
    "        axs[1,2].bar(key, f1[key])\n",
    "\n",
    "        axs[0,0].set_title(\"ROC Curve for \"+perc_TL+\"% TL\")\n",
    "#         axs[0].set_ylabel(\"Accuracy\")\n",
    "\n",
    "        axs[0,1].set_title(\"Test Accuracy for \"+perc_TL+\"% TL\")\n",
    "        axs[0,1].set_ylabel(\"Accuracy\")\n",
    "        axs[0,1].grid(axis='y', color='gray', linestyle='dashed')\n",
    "\n",
    "        axs[0,2].set_title(\"Train Accuracy for \"+perc_TL+\"% TL\")\n",
    "        axs[0,2].set_ylabel(\"Accuracy\")\n",
    "        axs[0,2].grid(axis='y', color='gray', linestyle='dashed')\n",
    "\n",
    "        axs[1,0].set_title(\"Recall for \"+perc_TL+\"% TL\")\n",
    "        axs[1,0].set_ylabel(\"Recall\")\n",
    "        axs[1,0].grid(axis='y', color='gray', linestyle='dashed')\n",
    "\n",
    "\n",
    "        axs[1,1].set_title(\"Precision \"+perc_TL+\"% TL\")\n",
    "        axs[1,1].set_ylabel(\"F1\")\n",
    "        axs[1,1].grid(axis='y', color='gray', linestyle='dashed')\n",
    "\n",
    "        axs[1,2].set_title(\"F1 \"+perc_TL+\"% TL\")\n",
    "        axs[1,2].set_ylabel(\"F1\")\n",
    "        axs[1,2].grid(axis='y', color='gray', linestyle='dashed')\n",
    "        \n",
    "        \n",
    "        [axs[y, x].tick_params(axis='x', labelrotation=60) for y in range(len(axs)) for x in range(len(axs[y]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Bar plot results  for testing and training using top 5 created features\n",
    "\n",
    "results_plt(X_train, y_train, X_test, y_test, \"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1_scores(args):\n",
    "    features_at_percent, percent, rep = args\n",
    "    df_at_percent = features_at_percent[percent]\n",
    "    df_at_percent = preprocessing(df_at_percent)\n",
    "    \n",
    "    try:\n",
    "        X_train, X_test, y_train, y_test  = create_split(df_at_percent, random_state=percent + (100 * rep))\n",
    "        model = RandomForestClassifier(min_samples_leaf = 5, class_weight = 'balanced_subsample',random_state=percent + (100 * rep))\n",
    "        model.fit(X_train,y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        return f1_score(y_test, predictions)        \n",
    "    except KeyError as e:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1_over_time_for_model(features_at_percent, repetitions=10):\n",
    "    f1_scores_mean = []\n",
    "    f1_scores_std = []\n",
    "    for percent in tqdm(range(1, 200), desc=\"Percent\"):\n",
    "        with mp.Pool(mp.cpu_count()) as pool:\n",
    "            f1_scores = pool.map(get_f1_scores, [(features_at_percent, percent, rep) for rep in range(repetitions)])\n",
    "            \n",
    "#             f1_scores = [get_f1_scores((features_at_percent, percent, rep)) for rep in range(repetitions)]\n",
    "\n",
    "            f1_scores_mean.append(np.mean(f1_scores))\n",
    "            f1_scores_std.append(np.std(f1_scores))\n",
    "        \n",
    "    return f1_scores_mean, f1_scores_std\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores_mean, f1_scores_std = get_f1_over_time_for_model(filtered_features_at_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.ylim((0, 1))\n",
    "y = np.array(f1_scores_mean)\n",
    "err = np.array(f1_scores_std)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,6)\n",
    "plt.plot(np.arange(0, 99.5, 0.5), y)\n",
    "plt.title(\"F1 score at varying percentages of TL\")\n",
    "plt.fill_between(np.arange(0, 99.5, 0.5), y - err, y + err, alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"f1_over_time.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems_available_at_percentage = [len(filtered_features_at_percent[i]) for i in range(1, 200)]\n",
    "plt.rcParams[\"figure.figsize\"] = (5,4)\n",
    "# plt.title(\"Problems unsolved after %\")\n",
    "plt.xlabel(\"Percentage of TL\")\n",
    "plt.ylabel(\"Prob. Unsolved\")\n",
    "plt.plot(np.arange(0, 99.5, 0.5), problems_available_at_percentage)\n",
    "plt.tight_layout()\n",
    "# plt.vlines([5], ymin=0, ymax=problems_available_at_percentage[10], color='black', alpha=0.5)\n",
    "plt.savefig(\"problems_avail_at_perc.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_reg = all_data[all_data.time_to_solution <  7199* 1000] #<2h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_at_percent_reg = {}\n",
    "for i in range (1,100):\n",
    "    df_i_reg=[]\n",
    "    for id, problem in all_data_reg.iterrows():\n",
    "            for index, p in enumerate(problem.statistics):\n",
    "                if index == i:\n",
    "                    new_p = dict(p['features'])\n",
    "                    new_p=cleanup(new_p)\n",
    "                    if i!=1:\n",
    "                        new_p=gradients(df_prev.loc[id], new_p)\n",
    "                    new_p['mzn'] = problem['mzn']\n",
    "                    new_p['dzn'] = problem['dzn']\n",
    "                    new_p['time_to_solution'] = problem['time_to_solution']\n",
    "                    df_i_reg.append((id, new_p))\n",
    "    df_i_reg = pd.DataFrame([a[1] for a in df_i_reg], index=[a[0] for a in df_i_reg])\n",
    "    df_i_reg=df_i_reg.fillna(value = 0)\n",
    "    if i!=0:   \n",
    "        features_at_percent_reg[i]=df_i_reg\n",
    "    df_prev=df_i_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_at_percent[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid=[5,10,20,50,90]\n",
    "for i in grid:\n",
    "    print(\"At\",i,\"%:\")\n",
    "    print(len(features_at_percent[i].index))\n",
    "    print(len(features_at_percent_reg[i].index))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg=features_at_percent_reg[1] #THE NUMBER HERE IS THE % OF TL\n",
    "df_reg=preprocessing(df_reg)\n",
    "# training-testing split\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg  = train_test_split(df_reg.drop(columns = [\"time_to_solution\"]),\\\n",
    "                                                     df_reg[\"time_to_solution\"], test_size=0.25, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RANDOM FOREST\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regr = RandomForestRegressor(max_depth=1000000000000)\n",
    "regr.fit(X_train_reg, y_train_reg)\n",
    "predictions=regr.predict(X_test_reg)\n",
    "sk_score = regr.score(X_test_reg, y_test_reg)\n",
    "sk_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SUPPORT VECTOR MACHINE\n",
    "from sklearn import svm\n",
    "regr = svm.SVR()\n",
    "regr.fit(X_train_reg, y_train_reg)\n",
    "predictions=regr.predict(X_test_reg)\n",
    "sk_score = regr.score(X_test_reg, y_test_reg)\n",
    "sk_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RIDGE\n",
    "from sklearn.linear_model import Ridge\n",
    "clf = Ridge(alpha=1.0)\n",
    "clf.fit(X_train_reg, y_train_reg)\n",
    "predictions=clf.predict(X_test_reg)\n",
    "sk_score = clf.score(X_test_reg, y_test_reg)\n",
    "sk_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LASSO\n",
    "from sklearn import linear_model\n",
    "clf = linear_model.Lasso(alpha=0.1)\n",
    "clf.fit(X_train_reg, y_train_reg)\n",
    "predictions=clf.predict(X_test_reg)\n",
    "sk_score = clf.score(X_test_reg, y_test_reg)\n",
    "sk_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ELASTICNET\n",
    "from sklearn.linear_model import ElasticNet\n",
    "regr = ElasticNet()\n",
    "regr.fit(X_train_reg, y_train_reg)\n",
    "predictions=regr.predict(X_test_reg)\n",
    "sk_score = regr.score(X_test_reg, y_test_reg)\n",
    "sk_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TREE\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeRegressor(max_depth=5)\n",
    "clf.fit(X_train_reg, y_train_reg)\n",
    "predictions=clf.predict(X_test_reg)\n",
    "sk_score = clf.score(X_test_reg, y_test_reg)\n",
    "sk_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLS\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "pls2 = PLSRegression(n_components=2)\n",
    "pls2.fit(X_train_reg, y_train_reg)\n",
    "predictions=pls2.predict(X_test_reg)\n",
    "sk_score = pls2.score(X_test_reg, y_test_reg)\n",
    "sk_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "kernel = DotProduct() + WhiteKernel()\n",
    "gpr = GaussianProcessRegressor(kernel=kernel).fit(X_train_reg, y_train_reg)\n",
    "predictions=gpr.predict(X_test_reg)\n",
    "sk_score = gpr.score(X_test_reg, y_test_reg)\n",
    "sk_score"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "colab": {
   "collapsed_sections": [],
   "name": "Analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "idmp",
   "language": "python",
   "name": "idmp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "090c59ccfcd44fc49b0d6b24987f0824c01aed8629dd5577dc9aff8c59f6d2bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
