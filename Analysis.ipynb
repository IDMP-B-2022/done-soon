{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done Soon? Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QA2OwOc1X3Fw"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QtKgh08BzA8s"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, RocCurveDisplay\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import f1_score\n",
    "import pymongo\n",
    "import plotly.express as px\n",
    "import math\n",
    "from enum import Enum\n",
    "from tqdm.notebook import tqdm\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstanceType(Enum):\n",
    "    ALL = 1\n",
    "    OPTIMIZATION = 2\n",
    "    SAT = 3\n",
    "\n",
    "data_types_to_consider = InstanceType.OPTIMIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data - Mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_instance_type(s):\n",
    "    return InstanceType.SAT if s == 'SAT' else InstanceType.OPTIMIZATION\n",
    "\n",
    "\n",
    "conn_str = f\"mongodb://localhost\"\n",
    "mongo_client = pymongo.MongoClient(conn_str, port=27017)\n",
    "\n",
    "#all data\n",
    "all_data = pd.DataFrame(mongo_client.done_soon.problems.find({}))\n",
    "all_data = all_data[all_data['error'] !=True ]\n",
    "all_data = all_data[all_data['problem_type'].map(string_to_instance_type) == data_types_to_consider ]\n",
    "\n",
    "#manual filtering (2hour)\n",
    "timeout_after_hour = all_data[all_data.time_to_solution > 7199 * 1000] #>2hours\n",
    "finishes_after_lower_bound = all_data[all_data.time_to_solution > 72 * 1000] #>72seconds\n",
    "\n",
    "#print result filtering / Sanity check\n",
    "print(f\"Unsolved: {len(timeout_after_hour)}\")\n",
    "print(f\"Class distribution: {len(timeout_after_hour)/(len(timeout_after_hour)+len(finishes_after_lower_bound))}\")\n",
    "\n",
    "#number of values of len of stats\n",
    "finishes_after_lower_bound.statistics.map(len).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_of_stats = np.array([len(x) for x in all_data['statistics']]) / 2\n",
    "time_to_solution = all_data['time_to_solution'] / 72000\n",
    "plt.scatter(length_of_stats, time_to_solution, marker='.')\n",
    "plt.title(\"Suspicious data (shouldn't this be linear?)\")\n",
    "plt.xlabel(\"length of stats array\")\n",
    "plt.ylabel(\"time to solution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some dataframe df\n",
    "all_data['time_to_solution'].sort_values().plot(use_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sub_data=all_data[all_data['time_to_solution']>7200]\n",
    "sub_data['time_to_solution'].sort_values().plot(use_index=False)\n",
    "#plt.plot(all_data[all_data['time_to_solution']>6000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "#finishes_after_lower_bound.statistics[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(df):\n",
    "    del df[\"decision_level_sat\"]\n",
    "    del df[\"ewma_decision_level_mip\"]\n",
    "    del df[\"decision_level_mip\"]\n",
    "#     del df[\"best_objective\"]\n",
    "#     df[\"unassnVar\"]   = (2**df['vars']) - df['opennodes']\n",
    "#     df[\"fracFailUnassn\"]     = df['conflicts'] / df['unassnVar']         # num failures/ num open nodes\n",
    "    df[\"fracOpenVisit\"]  = (df['vars'] - df['opennodes']) / df['opennodes']       # ratio of open nodes to visited nodes (how much of soln space explored)\n",
    "    df[\"fracBoolVars\"]     = df['boolVars'] / df['vars']                 # num bools / total num of vars\n",
    "    df[\"fracPropVars\"]     = df['propagations'] / df['vars']        # num propagations/ total num of vars\n",
    "#     df[\"frac_unassigned\"] = df['unassnVar'] / df['vars']  # current assignments/ total vars\n",
    "    df[\"fracLongClauses\"] = df['long'] + df['bin'] + df['tern']         # fraction of learnt clauses that have more than 3 literals\n",
    "    df[\"freqBackjumps\"]  = df['back_jumps']/df['search_time']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients(df_prev, df_curr):\n",
    "    keys=['conflicts','ewma_conflicts','decisions','search_iterations','opennodes','ewma_opennodes',\n",
    "          'vars','back_jumps','ewma_back_jumps','solutions','total_time','search_time','intVars',\n",
    "          'propagations','sat_propagations','ewma_propagations','propagators','boolVars','learnt',\n",
    "          'bin','tern','long','peak_depth','decision_level_engine','ewma_decision_level_engine',\n",
    "          'decision_level_treesize','clause_mem','prop_mem','ewma_best_objective',\n",
    "          'fracOpenVisit','fracBoolVars','fracPropVars','freqBackjumps', 'best_objective']\n",
    "    for i in keys:\n",
    "        df_curr[i+'_gradient']=(df_curr[i]-df_prev[i])/0.05*7200\n",
    "    return df_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_at_percent = {}\n",
    "\n",
    "for i in tqdm(range(1,200)):\n",
    "    df_i=[]\n",
    "    for id, problem in all_data.iterrows(): \n",
    "        for index, p in enumerate(problem.statistics):\n",
    "            if index == i:\n",
    "                new_p = dict(p['features'])\n",
    "                new_p=cleanup(new_p)\n",
    "                if i!=1:\n",
    "                    new_p=gradients(df_prev.loc[id], new_p)\n",
    "                new_p['mzn'] = problem['mzn']\n",
    "                new_p['dzn'] = problem['dzn']\n",
    "                new_p['solved_within_time_limit'] = problem['time_to_solution'] < 7199 * 1000 \\\n",
    "                or np.logical_not(np.isnan(problem['time_to_solution']))\n",
    "                df_i.append((id, new_p))\n",
    "    df_i = pd.DataFrame([a[1] for a in df_i], index=[a[0] for a in df_i])\n",
    "    df_i=df_i.fillna(value = 0)\n",
    "    if i!=0:   \n",
    "        features_at_percent[i]=df_i\n",
    "    df_prev=df_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans=features_at_percent[20].isna().sum().to_numpy().nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_at_percent[20].keys()[nans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_at_percent[20]\n",
    "print(len(features_at_percent[20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(features_at_percent[10]['ewma_best_objective'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMoP4AQWalT7"
   },
   "source": [
    "# Data Analysis\n",
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    df1=df.drop(['mzn','dzn'], axis=1)\n",
    "    df1.drop(df1.columns[df1.nunique() == 1], axis=1, inplace=True) #drop cols with constant value\n",
    "    #rescale data\n",
    "    transformer = MaxAbsScaler().fit(df1)\n",
    "    df1 = pd.DataFrame(transformer.transform(df1), columns=df1.columns, index=df1.index) #normalise data\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_split(df, test_size=0.25, random_state=22):\n",
    "    return train_test_split(df.drop(columns = [\"solved_within_time_limit\"]),\\\n",
    "                                df[\"solved_within_time_limit\"], test_size=0.25, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9hPn8jzm0U8c"
   },
   "outputs": [],
   "source": [
    "df=features_at_percent[20] #THE NUMBER HERE IS THE % OF TL\n",
    "df=preprocessing(df)\n",
    "# training-testing split\n",
    "X_train, X_test, y_train, y_test  = train_test_split(df.drop(columns = [\"solved_within_time_limit\"]),\\\n",
    "                                                     df[\"solved_within_time_limit\"], test_size=0.25, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IN-6LHsxsk1w",
    "outputId": "0a5afd18-e7fc-442c-eaa6-5fba63e0cd15"
   },
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6JaEQjH9sk1x",
    "outputId": "01f7b2d6-a07e-46fe-e868-ed9b02b23e4e"
   },
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUDr-kP-Yf_s"
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VcuEaofpzqWg"
   },
   "outputs": [],
   "source": [
    "models = {}\n",
    "\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "models['LR'] = LogisticRegression(max_iter=1000, C=1000 , class_weight = 'balanced',random_state=22)\n",
    "\n",
    "#Support Vector Machine\n",
    "from sklearn.svm import SVC\n",
    "models['SVM'] = SVC(kernel = 'rbf', class_weight = 'balanced', probability = True, random_state=22)\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "models['RF'] = RandomForestClassifier(min_samples_leaf = 5, class_weight = 'balanced_subsample',random_state=22)\n",
    "\n",
    "#Extra Tree\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "models['ET'] = ExtraTreesClassifier(class_weight = 'balanced', random_state=22)\n",
    "\n",
    "#Multi-layered perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "models['MLP'] = MLPClassifier(random_state=22)\n",
    "\n",
    "# Naive Bayes\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "#models['NB'] = GaussianNB()\n",
    "\n",
    "# Adaboost\n",
    "#from sklearn.ensemble import AdaBoostClassifier\n",
    "#models['AB'] = AdaBoostClassifier()\n",
    "\n",
    "#KNN\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "#models['KNN'] = KNeighborsClassifier(weights = 'distance')\n",
    "\n",
    "# Decision Trees\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "models['DT'] = DecisionTreeClassifier(max_depth = 5, class_weight = 'balanced', random_state=22)\n",
    "\n",
    "#Dummy classifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "models['DUM'] = DummyClassifier(strategy=\"most_frequent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicitions and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rf_reg = RandomForestClassifier(min_samples_leaf = 5)\n",
    "rf_reg.fit(X_train,y_train)\n",
    "\n",
    "predictions = rf_reg.predict(X_test)\n",
    "importances = rf_reg.feature_importances_\n",
    "\n",
    "print('The F1 score with all features of a RandomForestClassifier with min_samples_leaf of 5 is ', f1_score(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sorted_importance_indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.title('Feature Importance of Random Forest Regressor')\n",
    "plt.bar(range(len(sorted_importance_indices)), importances[sorted_importance_indices], align='center')\n",
    "plt.xticks(range(len(sorted_importance_indices)), X_train.columns[sorted_importance_indices], rotation=90)\n",
    "plt.rcParams[\"figure.figsize\"] = (20,3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 8 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sorted_importance_indices = np.argsort(importances)[::-1][:8]\n",
    "\n",
    "plt.title('Feature Importance of Random Forest Regressor')\n",
    "plt.bar(range(8), importances[sorted_importance_indices], align='center')\n",
    "plt.xticks(range(8), X_train.columns[sorted_importance_indices], rotation=90)\n",
    "plt.rcParams[\"figure.figsize\"] = (20,3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_importance_indices = np.argsort(importances)[::-1][:15]\n",
    "\n",
    "def f1_with_n_top_features(n, importance_column_indices):\n",
    "    summed = 0\n",
    "    for i in range(0, 10):\n",
    "        X_train, X_test, y_train, y_test  = train_test_split(df.drop(columns = [\"solved_within_time_limit\"]),\\\n",
    "                                                     df[\"solved_within_time_limit\"], test_size=0.25)\n",
    "        rf_reg = RandomForestClassifier(min_samples_leaf = 5)\n",
    "        rf_reg.fit(X_train.iloc[:, importance_column_indices[:n]], y_train)\n",
    "\n",
    "        predictions = rf_reg.predict(X_test.iloc[:, importance_column_indices[:n]])\n",
    "        summed += f1_score(y_test, predictions)\n",
    "    return summed / 10\n",
    "\n",
    "\n",
    "\n",
    "f1_scores = [(i, f1_with_n_top_features(i, sorted_importance_indices)) for i in range(1, len(sorted_importance_indices) + 1)[::-1]]\n",
    "\n",
    "plt.title(f'F1 score for top N features ({\"OPTIMIZATION\" if data_types_to_consider is InstanceType.OPTIMIZATION else \"SAT\"})')\n",
    "plt.plot([a[0] for a in f1_scores], [a[1] for a in f1_scores])\n",
    "plt.gca().set_ylim([0,1])\n",
    "plt.rcParams[\"figure.figsize\"] = (20,3)\n",
    "plt.show()\n",
    "\n",
    "print(f1_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_plt(X_train, y_train, X_test, y_test, perc_TL):\n",
    "    train_accuracy, accuracy, precision, recall, auc, f1 = {}, {}, {}, {}, {}, {}\n",
    "    figs, axs = plt.subplots(2,3,figsize=(20, 12))\n",
    "\n",
    "    for i, key in enumerate(models.keys()):\n",
    "\n",
    "        # Fit the classifier model\n",
    "        models[key].fit(X_train, y_train)\n",
    "\n",
    "        predictions = models[key].predict(X_test)\n",
    "        predictions_prob = models[key].predict_proba(X_test)[:,1]\n",
    "        train_predictions = models[key].predict(X_train)\n",
    "        # Predic  \n",
    "        # Calculate Accuracy, Precision and Recall Metrics\n",
    "        accuracy[key] = accuracy_score(predictions, y_test)\n",
    "        precision[key] = precision_score(predictions, y_test, zero_division=1)\n",
    "        recall[key] = recall_score(predictions, y_test, zero_division=1)\n",
    "        auc[key] = roc_auc_score(y_test, predictions_prob)\n",
    "        train_accuracy[key] = accuracy_score(train_predictions, y_train)\n",
    "        f1[key] = f1_score(y_test,predictions)\n",
    "        #should it be (true, pred)? yes\n",
    "\n",
    "        #To Display\n",
    "        RocCurveDisplay.from_predictions(y_test, predictions_prob, name=key , ax=axs[0,0])\n",
    "        axs[0,1].bar(key, accuracy[key]) \n",
    "        axs[0,2].bar(key, train_accuracy[key]) \n",
    "        axs[1,0].bar(key, recall[key])\n",
    "        axs[1,1].bar(key, precision[key])\n",
    "        axs[1,2].bar(key, f1[key])\n",
    "\n",
    "        axs[0,0].set_title(\"ROC Curve for \"+perc_TL+\"% TL\")\n",
    "#         axs[0].set_ylabel(\"Accuracy\")\n",
    "\n",
    "        axs[0,1].set_title(\"Test Accuracy for \"+perc_TL+\"% TL\")\n",
    "        axs[0,1].set_ylabel(\"Accuracy\")\n",
    "        axs[0,1].grid(axis='y', color='gray', linestyle='dashed')\n",
    "\n",
    "        axs[0,2].set_title(\"Train Accuracy for \"+perc_TL+\"% TL\")\n",
    "        axs[0,2].set_ylabel(\"Accuracy\")\n",
    "        axs[0,2].grid(axis='y', color='gray', linestyle='dashed')\n",
    "\n",
    "        axs[1,0].set_title(\"Recall for \"+perc_TL+\"% TL\")\n",
    "        axs[1,0].set_ylabel(\"Recall\")\n",
    "        axs[1,0].grid(axis='y', color='gray', linestyle='dashed')\n",
    "\n",
    "\n",
    "        axs[1,1].set_title(\"Precision \"+perc_TL+\"% TL\")\n",
    "        axs[1,1].set_ylabel(\"F1\")\n",
    "        axs[1,1].grid(axis='y', color='gray', linestyle='dashed')\n",
    "\n",
    "        axs[1,2].set_title(\"F1 \"+perc_TL+\"% TL\")\n",
    "        axs[1,2].set_ylabel(\"F1\")\n",
    "        axs[1,2].grid(axis='y', color='gray', linestyle='dashed')\n",
    "        \n",
    "        \n",
    "        [axs[y, x].tick_params(axis='x', labelrotation=60) for y in range(len(axs)) for x in range(len(axs[y]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Bar plot results  for testing and training using top 5 created features\n",
    "\n",
    "results_plt(X_train, y_train, X_test, y_test, \"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1_scores(args):\n",
    "    features_at_percent, percent, rep = args\n",
    "    df_at_percent = features_at_percent[percent]\n",
    "    df_at_percent = preprocessing(df_at_percent)\n",
    "\n",
    "    X_train, X_test, y_train, y_test  = create_split(df_at_percent, random_state=percent + (100 * rep))\n",
    "    model = RandomForestClassifier(min_samples_leaf = 5, class_weight = 'balanced_subsample',random_state=percent + (100 * rep))\n",
    "    model.fit(X_train,y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    return f1_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1_over_time_for_model(features_at_percent, repetitions=10):\n",
    "    f1_scores_mean = []\n",
    "    f1_scores_std = []\n",
    "    for percent in tqdm(range(1, 200), desc=\"Percent\"):\n",
    "        with mp.Pool(mp.cpu_count()) as pool:\n",
    "            f1_scores = list(tqdm(pool.imap(get_f1_scores,\n",
    "                                            [(features_at_percent, percent, rep) for rep in range(repetitions)]),\n",
    "                                  total=10, leave=False, desc=\"Repetitions\"))\n",
    "\n",
    "            f1_scores_mean.append(np.mean(f1_scores))\n",
    "            f1_scores_std.append(np.std(f1_scores))\n",
    "        \n",
    "    return f1_scores_mean, f1_scores_std\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores_mean, f1_scores_std = get_f1_over_time_for_model(features_at_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.ylim((0, 1))\n",
    "y = np.array(f1_scores_mean)\n",
    "err = np.array(f1_scores_std)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,6)\n",
    "plt.plot(np.arange(0, 99.5, 0.5), y)\n",
    "plt.title(\"F1 score at varying percentages of TL\")\n",
    "plt.fill_between(np.arange(0, 99.5, 0.5), y - err, y + err, alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"f1_over_time.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems_available_at_percentage = [len(features_at_percent[i]) for i in range(1, 200)]\n",
    "plt.rcParams[\"figure.figsize\"] = (3,2)\n",
    "# plt.title(\"Problems unsolved after %\")\n",
    "plt.xlabel(\"Percentage of TL\")\n",
    "plt.ylabel(\"Prob. Unsolved\")\n",
    "plt.plot(np.arange(0, 99.5, 0.5), problems_available_at_percentage)\n",
    "plt.tight_layout()\n",
    "plt.vlines([5], ymin=0, ymax=problems_available_at_percentage[10], color='black', alpha=0.5)\n",
    "plt.savefig(\"problems_avail_at_perc.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems_available_at_percentage[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_reg = all_data[all_data.time_to_solution <  7199* 1000] #<2h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_at_percent_reg = {}\n",
    "for i in range (1,100):\n",
    "    df_i_reg=[]\n",
    "    for id, problem in all_data_reg.iterrows():\n",
    "            for index, p in enumerate(problem.statistics):\n",
    "                if index == i:\n",
    "                    new_p = dict(p['features'])\n",
    "                    new_p=cleanup(new_p)\n",
    "                    if i!=1:\n",
    "                        new_p=gradients(df_prev.loc[id], new_p)\n",
    "                    new_p['mzn'] = problem['mzn']\n",
    "                    new_p['dzn'] = problem['dzn']\n",
    "                    new_p['time_to_solution'] = problem['time_to_solution']\n",
    "                    df_i_reg.append((id, new_p))\n",
    "    df_i_reg = pd.DataFrame([a[1] for a in df_i_reg], index=[a[0] for a in df_i_reg])\n",
    "    df_i_reg=df_i_reg.fillna(value = 0)\n",
    "    if i!=0:   \n",
    "        features_at_percent_reg[i]=df_i_reg\n",
    "    df_prev=df_i_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_at_percent[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid=[5,10,20,50,90]\n",
    "for i in grid:\n",
    "    print(\"At\",i,\"%:\")\n",
    "    print(len(features_at_percent[i].index))\n",
    "    print(len(features_at_percent_reg[i].index))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg=features_at_percent_reg[1] #THE NUMBER HERE IS THE % OF TL\n",
    "df_reg=preprocessing(df_reg)\n",
    "# training-testing split\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg  = train_test_split(df_reg.drop(columns = [\"time_to_solution\"]),\\\n",
    "                                                     df_reg[\"time_to_solution\"], test_size=0.25, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RANDOM FOREST\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regr = RandomForestRegressor(max_depth=1000000000000)\n",
    "regr.fit(X_train_reg, y_train_reg)\n",
    "predictions=regr.predict(X_test_reg)\n",
    "sk_score = regr.score(X_test_reg, y_test_reg)\n",
    "sk_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SUPPORT VECTOR MACHINE\n",
    "from sklearn import svm\n",
    "regr = svm.SVR()\n",
    "regr.fit(X_train_reg, y_train_reg)\n",
    "predictions=regr.predict(X_test_reg)\n",
    "sk_score = regr.score(X_test_reg, y_test_reg)\n",
    "sk_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RIDGE\n",
    "from sklearn.linear_model import Ridge\n",
    "clf = Ridge(alpha=1.0)\n",
    "clf.fit(X_train_reg, y_train_reg)\n",
    "predictions=clf.predict(X_test_reg)\n",
    "sk_score = clf.score(X_test_reg, y_test_reg)\n",
    "sk_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LASSO\n",
    "from sklearn import linear_model\n",
    "clf = linear_model.Lasso(alpha=0.1)\n",
    "clf.fit(X_train_reg, y_train_reg)\n",
    "predictions=clf.predict(X_test_reg)\n",
    "sk_score = clf.score(X_test_reg, y_test_reg)\n",
    "sk_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ELASTICNET\n",
    "from sklearn.linear_model import ElasticNet\n",
    "regr = ElasticNet()\n",
    "regr.fit(X_train_reg, y_train_reg)\n",
    "predictions=regr.predict(X_test_reg)\n",
    "sk_score = regr.score(X_test_reg, y_test_reg)\n",
    "sk_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TREE\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeRegressor(max_depth=5)\n",
    "clf.fit(X_train_reg, y_train_reg)\n",
    "predictions=clf.predict(X_test_reg)\n",
    "sk_score = clf.score(X_test_reg, y_test_reg)\n",
    "sk_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLS\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "pls2 = PLSRegression(n_components=2)\n",
    "pls2.fit(X_train_reg, y_train_reg)\n",
    "predictions=pls2.predict(X_test_reg)\n",
    "sk_score = pls2.score(X_test_reg, y_test_reg)\n",
    "sk_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "kernel = DotProduct() + WhiteKernel()\n",
    "gpr = GaussianProcessRegressor(kernel=kernel).fit(X_train_reg, y_train_reg)\n",
    "predictions=gpr.predict(X_test_reg)\n",
    "sk_score = gpr.score(X_test_reg, y_test_reg)\n",
    "sk_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ignore the rest! \n",
    "# -- -- -- -- -- -- \n",
    "# -- -- -- -- -- -- \n",
    "# -- -- -- -- -- -- \n",
    "# -- -- -- -- -- -- \n",
    "# -- -- -- -- -- -- \n",
    "# -- -- -- -- -- -- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Ftayu36YnrR"
   },
   "source": [
    "# **Feature Importance Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzOOxMce5WK7"
   },
   "source": [
    "Feature importance for all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6XZEREqH1U9l",
    "outputId": "1f10cec3-98e3-49a5-bd26-cc3e02cb70f3"
   },
   "outputs": [],
   "source": [
    "def feat_import_plt(X, y, perc_TL):\n",
    "    #Display Flags \n",
    "    ordered = False   #True displays importances in descending order\n",
    "    errorbar = True\n",
    "    cardinality = False\n",
    "    \n",
    "    #Drop values that are neither standard chuffed values nor created\n",
    "    X = X.drop([\"solutions\",\n",
    "                \"total_time\",\n",
    "                \"search_time\", #total time - init time\n",
    "                \"learnt\" ,\n",
    "                \"bin\",\n",
    "                \"tern\",\n",
    "                \"long\",\n",
    "                \"ewma_opennodes\",\n",
    "                \"ewma_decision_level_engine\",\n",
    "                \"ewma_back_jumps\",\n",
    "                \"ewma_propagations\",\n",
    "                \"ewma_best_objective\"], axis=1, inplace=False)\n",
    "\n",
    "    #Feature importance using Gini Impurity\n",
    "    dcs_t = RandomForestClassifier(min_samples_leaf = 5,random_state=22)\n",
    "    dcs_t.fit(X, y)\n",
    "    \n",
    "    importances = dcs_t.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in dcs_t.estimators_], axis=0) #get standard dev of importances\n",
    "    \n",
    "    if ordered:\n",
    "        _sorted_indices = np.argsort(importances)[::-1]   # Sort the feature importance in descending order\n",
    "    else:\n",
    "        _sorted_indices = np.array(range(len(importances)))   # unsorted feature importances for comparison\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    ax1 = plt.subplot(1, 2, 1)\n",
    "    plt.title('Gini Importance using Random Forest for '+ perc_TL+'% TL with normalized feature cardinality')\n",
    "    \n",
    "    if not errorbar:\n",
    "        ax1.bar(range(X.shape[1]), importances[_sorted_indices])\n",
    "    else :\n",
    "        ax1.bar(range(X.shape[1]), importances[_sorted_indices], yerr=std, ecolor='black', capsize=5, align='center')\n",
    "        ax1.set_ylabel('Importance')\n",
    "        plt.xticks(range(X.shape[1]), X.columns[_sorted_indices], rotation=90)\n",
    "\n",
    "    #print(\"GI:\", importances[sorted_indices])\n",
    "\n",
    "    if cardinality:\n",
    "        c = df_5.apply(pd.Series.nunique) #feature cardinalityc_array = []\n",
    "    for i in X.columns[_sorted_indices]:\n",
    "        c_array.append(c.loc[i]/ max(df_5.apply(pd.Series.nunique).values))\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(range(X.shape[1]), c_array, color = 'r' )\n",
    "    ax2.set_ylabel('Cardinality', color = 'r')\n",
    "\n",
    "    #Feature importance using Permutation Importance\n",
    "    perm = permutation_importance(dcs_t, X, y, random_state = 22)\n",
    "    if ordered:\n",
    "        _perm_sorted_indices = np.argsort(perm.importances_mean)[::-1] # Sort the feature importance in descending order\n",
    "    else:\n",
    "        _perm_sorted_indices = _sorted_indices\n",
    "\n",
    "    ax3 = plt.subplot(1, 2, 2)\n",
    "    plt.title('Permutation Importance using Random Forest for '+ perc_TL+'% TL')\n",
    "    if not errorbar:\n",
    "        ax3.bar(range(X.shape[1]), perm.importances_mean[_perm_sorted_indices]*10)\n",
    "    else:\n",
    "        ax3.bar(range(X.shape[1]), perm.importances_mean[_perm_sorted_indices]*10, yerr=perm.importances_std, ecolor='black', capsize=5, align='center')\n",
    "    plt.xticks(range(X.shape[1]), X.columns[_perm_sorted_indices], rotation=90)\n",
    "    ax3.set_ylabel('Importance')\n",
    "\n",
    "    #print(\"PI:\", perm.importances_mean[perm_sorted_indices])\n",
    "\n",
    "    if cardinality:\n",
    "        c_array = []\n",
    "        for i in X.columns[_perm_sorted_indices]:\n",
    "            c_array.append(c.loc[i]/ max(df_5.apply(pd.Series.nunique).values))\n",
    "        ax2 = ax3.twinx()\n",
    "        ax2.plot(range(X.shape[1]), c_array, color = 'r' )\n",
    "        ax2.set_ylabel('Cardinality', color = 'r')\n",
    "\n",
    "    #plt.subplot(1, 3, 3)\n",
    "    #sns.heatmap(X.corr(), mask = np.triu(X.corr()) )\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Large error bars may result from Gini Importance being misleading when features have many unique values \\n\")\n",
    "feat_import_plt(X_train, y_train,  \"5\")\n",
    "# feat_import_plt(X_train_10, y_train_10, \"10\")\n",
    "# feat_import_plt(X_train_15, y_train_15,  \"15\")\n",
    "# feat_import_plt(X_train_20, y_train_20,  \"20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58S764KD5eJ0"
   },
   "source": [
    "Feature importance for proposed (or created) features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 823
    },
    "id": "tte0xg5MTpx2",
    "outputId": "11a5aa66-4f73-4244-d50e-7a517a0ce329"
   },
   "outputs": [],
   "source": [
    "#Only keep proposed features\n",
    "for df in df_list:\n",
    "    df.drop([\"opennodes\",\n",
    "             \"ewma_opennodes\",\n",
    "             \"ewma_decision_level_engine\",\n",
    "             \"vars\",\n",
    "             \"back_jumps\",\n",
    "             \"ewma_back_jumps\",\n",
    "             \"solutions\",\n",
    "             \"total_time\",\n",
    "             \"search_time\", #total time - init time\n",
    "             \"intVars\",\n",
    "             \"propagations\",\n",
    "             \"ewma_propagations\",\n",
    "             \"propagators\",\n",
    "             \"boolVars\",\n",
    "             \"learnt\" ,\n",
    "             \"bin\",\n",
    "             \"tern\",\n",
    "             \"long\",\n",
    "             \"peak_depth\",\n",
    "             \"best_objective\",\n",
    "             \"ewma_best_objective\"], axis=1, inplace=True)\n",
    "  \n",
    "  \n",
    "  \n",
    "#dummy vars for df_5 to match size of other dfs\n",
    "df_5[\"objectiveGROC\"] = np.zeros(df_5.shape[0])\n",
    "df_5[\"failuresGROC\"] = np.zeros(df_5.shape[0])\n",
    "df_5[\"backjumpsGROC\"] = np.zeros(df_5.shape[0])\n",
    "df_5[\"propagationsGROC\"] = np.zeros(df_5.shape[0])\n",
    "df_5[\"solutionsGROC\"] = np.zeros(df_5.shape[0])\n",
    "\n",
    "\n",
    "# Resplit dats into training-testing \n",
    "Xd_train_5, Xd_test_5, yd_train_5, yd_test_5  = train_test_split(df_5.drop(columns = [\"solved_within_time_limit\"]),\\\n",
    "                                                     df_5[\"solved_within_time_limit\"], test_size=0.25, random_state=22)\n",
    "\n",
    "# Xd_train_10, Xd_test_10, yd_train_10, yd_test_10  = train_test_split(df_10.drop(columns = [\"solved_within_time_limit\"]),\\\n",
    "#                                                      df_10[\"solved_within_time_limit\"], test_size=0.25, random_state=22)\n",
    "\n",
    "# Xd_train_15, Xd_test_15, yd_train_15, yd_test_15  = train_test_split(df_15.drop(columns = [\"solved_within_time_limit\"]),\\\n",
    "#                                                      df_15[\"solved_within_time_limit\"], test_size=0.25, random_state=22)\n",
    "\n",
    "# Xd_train_20, Xd_test_20, yd_train_20, yd_test_20  = train_test_split(df_20.drop(columns = [\"solved_within_time_limit\"]),\\\n",
    "#                                                      df_20[\"solved_within_time_limit\"], test_size=0.25, random_state=22)\n",
    "# #Feature importances using only proposed features\n",
    "def importance (X, y):\n",
    "  dcs_5 = RandomForestClassifier(min_samples_leaf = 5,random_state=22)\n",
    "  dcs_5.fit(X, y)\n",
    "  importances_5 = dcs_5.feature_importances_\n",
    "  std_5 = np.std([tree.feature_importances_ for tree in dcs_5.estimators_], axis=0) #get standard dev of importances\n",
    "  perm_5 = permutation_importance(dcs_5, X, y, random_state = 22)\n",
    "  return importances_5, perm_5\n",
    "\n",
    "[importances_5, perm_5] = importance(Xd_train_5, yd_train_5)\n",
    "sorted_indices = np.argsort(importances_5)[::-1] \n",
    "perm_sorted_indices = np.argsort(perm_5.importances_mean)[::-1] \n",
    "\n",
    "# [importances_10, perm_10] = importance(Xd_train_10, yd_train_10)\n",
    "# sorted_indices_10 = np.argsort(importances_10)[::-1] \n",
    "# perm_sorted_indices_10 = np.argsort(perm_10.importances_mean)[::-1]\n",
    "\n",
    "# [importances_15, perm_15] = importance(Xd_train_15, yd_train_15)\n",
    "# sorted_indices_15 = np.argsort(importances_15)[::-1] \n",
    "# perm_sorted_indices_15 = np.argsort(perm_15.importances_mean)[::-1] \n",
    "\n",
    "# [importances_20, perm_20] = importance(Xd_train_20, yd_train_20)\n",
    "# sorted_indices_20 = np.argsort(importances_20)[::-1] \n",
    "# perm_sorted_indices_20 = np.argsort(perm_20.importances_mean)[::-1] \n",
    "\n",
    "#Print images\n",
    "printAll = True #set to flase to show top 10 features\n",
    "\n",
    "if printAll:\n",
    "  plt.figure(figsize=(7, 5))\n",
    "  plt.title('Gini Importance of Created Features for Varying % TL')\n",
    "  plt.plot(Xd_train_5.columns[sorted_indices][0:9], importances_5[sorted_indices][0:9],'-o' )\n",
    "#   plt.plot(Xd_train_10.columns[sorted_indices], importances_10[sorted_indices], '-s')\n",
    "#   plt.plot(Xd_train_15.columns[sorted_indices], importances_15[sorted_indices], '-^' )\n",
    "#   plt.plot(Xd_train_20.columns[sorted_indices], importances_20[sorted_indices], '-v')\n",
    "  plt.axhline(y=0.10, linestyle='--')\n",
    "  plt.xticks(range(Xd_train_5.shape[1]), Xd_train_5.columns[sorted_indices], rotation=90)\n",
    "  plt.ylabel('Importance')\n",
    "  plt.grid(axis='both', color='gray', linestyle='dashed')\n",
    "  plt.legend([\"5% TL\", \"10% TL\", \"15% TL\", \"20% TL\"])\n",
    "\n",
    "  plt.figure(figsize=(7, 5))\n",
    "  plt.title('Permutation Importance of Created Features for Varying % TL')\n",
    "  plt.plot(Xd_train_5.columns[sorted_indices][0:9], perm_5.importances_mean[sorted_indices][0:9]*10,'-o' )\n",
    "#   plt.plot(Xd_train_10.columns[sorted_indices], perm_10.importances_mean[sorted_indices]*10, '-s')\n",
    "#   plt.plot(Xd_train_15.columns[sorted_indices], perm_15.importances_mean[sorted_indices]*10, '-^' )\n",
    "#   plt.plot(Xd_train_20.columns[sorted_indices], perm_20.importances_mean[sorted_indices]*10, '-v')\n",
    "  plt.axhline(y=0.07, linestyle='--')\n",
    "  plt.xticks(range(Xd_train_5.shape[1]), Xd_train_5.columns[sorted_indices], rotation=90)\n",
    "  plt.ylabel('Importance')\n",
    "  plt.grid(axis='both', color='gray', linestyle='dashed')\n",
    "  plt.legend([\"5% TL\", \"10% TL\", \"15% TL\", \"20% TL\"])\n",
    "else:\n",
    "  plt.figure(figsize=(7, 5))\n",
    "  plt.title('Top 10 Gini Important Created Features for Varying % TL')\n",
    "  plt.plot(Xd_train_5.columns[sorted_indices][0:10], importances_5[sorted_indices][0:10],'o' )\n",
    "  plt.plot(Xd_train_10.columns[sorted_indices_10][0:10], importances_10[sorted_indices_10][0:10], 's')\n",
    "  plt.plot(Xd_train_15.columns[sorted_indices_15][0:10], importances_15[sorted_indices_15][0:10], '^' )\n",
    "  plt.plot(Xd_train_20.columns[sorted_indices_20][0:10], importances_20[sorted_indices_20][0:10], 'v')\n",
    "  plt.axhline(y=0.10, linestyle='--')\n",
    "  plt.xticks(range(13), Xd_train_5.columns[sorted_indices][0:12], rotation=90)\n",
    "  plt.ylabel('Importance')\n",
    "  plt.grid(axis='both', color='gray', linestyle='dashed')\n",
    "  plt.legend([\"5% TL\", \"10% TL\", \"15% TL\", \"20% TL\"])\n",
    "\n",
    "  plt.figure(figsize=(7, 5))\n",
    "  plt.title('Top 10 Permutation Important Created Features for Varying % TL')\n",
    "  plt.plot(Xd_train_5.columns[perm_sorted_indices][0:10], perm_5.importances_mean[perm_sorted_indices][0:10]*10,'o' )\n",
    "  plt.plot(Xd_train_10.columns[perm_sorted_indices_10][0:10], perm_10.importances_mean[perm_sorted_indices_10][0:10]*10, 's')\n",
    "  plt.plot(Xd_train_15.columns[perm_sorted_indices_15][0:10], perm_15.importances_mean[perm_sorted_indices_15][0:10]*10, '^' )\n",
    "  plt.plot(Xd_train_20.columns[perm_sorted_indices_20][0:10], perm_20.importances_mean[perm_sorted_indices_20][0:10]*10, 'v')\n",
    "  plt.axhline(y=0.07, linestyle='--')\n",
    "  plt.xticks(range(13), Xd_train_5.columns[perm_sorted_indices][0:12], rotation=90)\n",
    "  plt.ylabel('Importance')\n",
    "  plt.grid(axis='both', color='gray', linestyle='dashed')\n",
    "  plt.legend([\"5% TL\", \"10% TL\", \"15% TL\", \"20% TL\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZX89QNko2uDH",
    "outputId": "5f228fbe-9f78-4631-fd79-9f0fb47da7a2"
   },
   "outputs": [],
   "source": [
    "#select top 5 features for training\n",
    "Xs_train_5  = Xd_train_5.iloc[:, sorted_indices[0:5]]\n",
    "# Xs_train_10 = Xd_train_10.iloc[:,sorted_indices_10[0:5]]\n",
    "# Xs_train_15 = Xd_train_15.iloc[:,sorted_indices_15[0:5]]\n",
    "# Xs_train_20 = Xd_train_20.iloc[:,sorted_indices_20[0:5]]\n",
    "\n",
    "Xs_test_5  = Xd_test_5.iloc[:,sorted_indices[0:5]]\n",
    "# Xs_test_10 = Xd_test_10.iloc[:,sorted_indices_10[0:5]]\n",
    "# Xs_test_15 = Xd_test_15.iloc[:,sorted_indices_15[0:5]]\n",
    "# Xs_test_20 = Xd_test_20.iloc[:,sorted_indices_20[0:5]]\n",
    "\n",
    "print(\"Top 5 features:\",Xd_train_5.columns[sorted_indices][0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4P8fHs8Yss9"
   },
   "source": [
    "# **Predictions and Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6VbesRDj0L8u"
   },
   "outputs": [],
   "source": [
    "def results_plt(X_train, y_train, X_test, y_test, perc_TL):\n",
    "    train_accuracy, accuracy, precision, recall, auc, f1 = {}, {}, {}, {}, {}, {}\n",
    "    figs, axs = plt.subplots(1,6,figsize=(35, 5))\n",
    "\n",
    "    for i, key in enumerate(models.keys()):\n",
    "\n",
    "        # Fit the classifier model\n",
    "        models[key].fit(X_train, y_train)\n",
    "\n",
    "        predictions = models[key].predict(X_test)\n",
    "        predictions_prob = models[key].predict_proba(X_test)[:,1]\n",
    "        train_predictions = models[key].predict(X_train)\n",
    "        # Predic  \n",
    "        # Calculate Accuracy, Precision and Recall Metrics\n",
    "        accuracy[key] = accuracy_score(predictions, y_test)\n",
    "        precision[key] = precision_score(predictions, y_test, zero_division=1)\n",
    "        recall[key] = recall_score(predictions, y_test, zero_division=1)\n",
    "        auc[key] = roc_auc_score(y_test, predictions_prob)\n",
    "        train_accuracy[key] = accuracy_score(train_predictions, y_train)\n",
    "        f1[key] = f1_score(y_test,predictions)\n",
    "        #should it be (true, pred)? yes\n",
    "\n",
    "        #To Display\n",
    "        RocCurveDisplay.from_predictions(y_test, predictions_prob, name=key , ax=axs[0])\n",
    "        axs[1].bar(key, accuracy[key]) \n",
    "        axs[2].bar(key, train_accuracy[key]) \n",
    "        axs[3].bar(key, recall[key])\n",
    "        axs[4].bar(key, precision[key])\n",
    "        axs[5].bar(key, f1[key])\n",
    "\n",
    "        axs[0].set_title(\"ROC Curve for \"+perc_TL+\"% TL\")\n",
    "        #axs[0].set_ylabel(\"Accuracy\")\n",
    "\n",
    "        axs[1].set_title(\"Test Accuracy for \"+perc_TL+\"% TL\")\n",
    "        axs[1].set_xticklabels(models.keys(),rotation=60)\n",
    "        axs[1].set_ylabel(\"Accuracy\")\n",
    "        axs[1].grid(axis='y', color='gray', linestyle='dashed')\n",
    "\n",
    "        axs[2].set_title(\"Train Accuracy for \"+perc_TL+\"% TL\")\n",
    "        axs[2].set_xticklabels(models.keys(),rotation=60)\n",
    "        axs[2].set_ylabel(\"Accuracy\")\n",
    "        axs[2].grid(axis='y', color='gray', linestyle='dashed')\n",
    "\n",
    "        axs[3].set_title(\"Recall for \"+perc_TL+\"% TL\")\n",
    "        axs[3].set_xticklabels(models.keys(),rotation=60)\n",
    "        axs[3].set_ylabel(\"Recall\")\n",
    "        axs[3].grid(axis='y', color='gray', linestyle='dashed')\n",
    "\n",
    "\n",
    "        axs[4].set_title(\"Precision \"+perc_TL+\"% TL\")\n",
    "        axs[4].set_xticklabels(models.keys(),rotation=60)\n",
    "        axs[4].set_ylabel(\"F1\")\n",
    "        axs[4].grid(axis='y', color='gray', linestyle='dashed')\n",
    "\n",
    "        axs[5].set_title(\"F1 \"+perc_TL+\"% TL\")\n",
    "        axs[5].set_xticklabels(models.keys(),rotation=60)\n",
    "        axs[5].set_ylabel(\"F1\")\n",
    "        axs[5].grid(axis='y', color='gray', linestyle='dashed')\n",
    "        figs.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 693
    },
    "id": "2MJtiiDs3bL0",
    "outputId": "b6053268-c521-4a67-ddf6-dd72bd2330b8"
   },
   "outputs": [],
   "source": [
    "#Bar plot results  for testing and training using top 5 created features\n",
    "results_plt(X_train, y_train, X_test, y_test, \"40\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4nuHcJnUKAOc"
   },
   "outputs": [],
   "source": [
    "def results_line_plt(X_train, y_train, X_test, y_test, perc_TL):\n",
    "  train_accuracy, accuracy, precision, recall, auc, f1 = {}, {}, {}, {}, {}, {}\n",
    "  #figs, axs = plt.subplots(1,6,figsize=(35, 5))\n",
    "\n",
    "  for i, key in enumerate(models.keys()):\n",
    "\n",
    "          # Fit the classifier model\n",
    "          models[key].fit(X_train, y_train)\n",
    "\n",
    "          # Prediction\n",
    "          predictions = models[key].predict(X_test)\n",
    "          predictions_prob = models[key].predict_proba(X_test)[:,1]\n",
    "          train_predictions = models[key].predict(X_train)\n",
    "\n",
    "          # Calculate Accuracy, Precision and Recall Metrics\n",
    "          accuracy[key] = accuracy_score(predictions, y_test)\n",
    "          precision[key] = precision_score(predictions, y_test, zero_division=1)\n",
    "          recall[key] = recall_score(predictions, y_test, zero_division=1)\n",
    "          auc[key] = roc_auc_score(y_test, predictions_prob)\n",
    "          train_accuracy[key] = accuracy_score(train_predictions, y_train)\n",
    "          f1[key] = f1_score(y_test,predictions)\n",
    "          #should it be (true, pred)? yes\n",
    "  print(\"\\n------------------------------------------------------------------------\")\n",
    "  print(perc_TL+\"% TL train acc:\", np.round(list(train_accuracy.values()),2))\n",
    "  print(perc_TL+\"% TL test acc :\", np.round(list(accuracy.values()),2))\n",
    "  print(perc_TL+\"% TL precision:\",  np.round(list(precision.values()),2))\n",
    "  print(perc_TL+\"% TL recall   :\",  np.round(list(recall.values()),2))\n",
    "  print(perc_TL+\"% TL F1       :\",  np.round(list(f1.values()),2))\n",
    "  print(perc_TL+\"% TL ROC AUC  :\",  np.round(list(auc.values()),2))\n",
    "  print(\"------------------------------------------------------------------------\")\n",
    "  return train_accuracy, accuracy, precision, recall, auc, f1\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fS7Jmx-qKSby",
    "outputId": "f5dc40e8-c4f0-448c-9669-f6f8bfbfe918"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Line plot results  for testing and training using top 5 created features\n",
    "train_accuracy_5, accuracy_5, precision_5, recall_5, auc_5, f1_5 = results_line_plt(Xs_train_5, yd_train_5, Xs_test_5, yd_test_5, \"5\")\n",
    "# train_accuracy_10, accuracy_10, precision_10, recall_10, auc_10, f1_10 = results_line_plt(Xs_train_10, yd_train_10, Xs_test_10, yd_test_10, \"10\")\n",
    "# train_accuracy_15, accuracy_15, precision_15, recall_15, auc_15, f1_15 = results_line_plt(Xs_train_15, yd_train_15, Xs_test_15, yd_test_15, \"15\")\n",
    "# train_accuracy_20, accuracy_20, precision_20, recall_20, auc_20, f1_20 = results_line_plt(Xs_train_20, yd_train_20, Xs_test_20, yd_test_20, \"20\")\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.title(\"Testing Accuracy for Varying %TL\")\n",
    "plt.plot(list(accuracy_5.keys()), list(accuracy_5.values()), '-o') \n",
    "# plt.plot(list(accuracy_10.keys()), list(accuracy_10.values()), '-s') \n",
    "# plt.plot(list(accuracy_15.keys()), list(accuracy_15.values()), '-^') \n",
    "# plt.plot(list(accuracy_20.keys()), list(accuracy_20.values()), '-v') \n",
    "plt.axhline(y=1, linestyle='--')\n",
    "plt.xticks(range(len(models)), models.keys(),rotation=60)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(axis='both', color='gray', linestyle='dashed')\n",
    "plt.legend([\"5% TL\"]) #, \"10% TL\", \"15% TL\", \"20% TL\"])\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.title(\"Training Accuracy for Varying %TL\")\n",
    "plt.plot(list(train_accuracy_5.keys()), list(train_accuracy_5.values()),'-o') \n",
    "# plt.plot(list(train_accuracy_10.keys()), list(train_accuracy_10.values()), '-s') \n",
    "# plt.plot(list(train_accuracy_15.keys()), list(train_accuracy_15.values()), '-^') \n",
    "# plt.plot(list(train_accuracy_20.keys()), list(train_accuracy_20.values()), '-v') \n",
    "plt.axhline(y=1, linestyle='--')\n",
    "plt.xticks(range(len(models)), models.keys(),rotation=60)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(axis='both', color='gray', linestyle='dashed')\n",
    "plt.legend([\"5% TL\"]) #, \"10% TL\", \"15% TL\", \"20% TL\"])\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.title(\"F1 score for Varying %TL\")\n",
    "plt.plot(list(f1_5.keys()), list(f1_5.values()),'-o') \n",
    "# plt.plot(list(f1_10.keys()), list(f1_10.values()), '-s') \n",
    "# plt.plot(list(f1_15.keys()), list(f1_15.values()), '-^') \n",
    "# plt.plot(list(f1_20.keys()), list(f1_20.values()), '-v') \n",
    "plt.axhline(y=1, linestyle='--')\n",
    "plt.xticks(range(len(models)), models.keys(),rotation=60)\n",
    "plt.ylabel('F1')\n",
    "plt.grid(axis='both', color='gray', linestyle='dashed')\n",
    "plt.legend([\"5% TL\"]) #, \"10% TL\", \"15% TL\", \"20% TL\"])\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 622
    },
    "id": "RMqT6Bjxsk13",
    "outputId": "e571f90e-2171-46b2-c695-ee3097f467f0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cy-3klfLsk14"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "#confusion_matrix(y_test_5,predictions)\n",
    "plot_confusion_matrix(rf_reg, X_test_20, y_test_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mgTPoTw7sk14"
   },
   "outputs": [],
   "source": [
    "df_20.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2P1rWHWOsk15"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf = RandomForestClassifier(min_samples_leaf = 5)\n",
    "scores = cross_val_score(rf, df_20.drop('solved_within_time_limit', axis=1), df_20['solved_within_time_limit'], cv=5, scoring='f1')\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tca5kt3fsk15"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "X = df_20.drop('solved_within_time_limit', axis=1)\n",
    "y = df_20['solved_within_time_limit']\n",
    "for train, test in skf.split(X, y):\n",
    "    rf = RandomForestClassifier(min_samples_leaf = 5)\n",
    "    rf.fit(X.iloc[train], y.iloc[train])\n",
    "    pred = rf.predict(X.iloc[test])\n",
    "    print(len(y.iloc[test][y == 1]))\n",
    "    print(f1_score(y.iloc[test], pred))\n",
    "    plot_confusion_matrix(rf, X.iloc[test], y.iloc[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1eXMMZgUsk16"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "colab": {
   "collapsed_sections": [],
   "name": "Analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "idmp",
   "language": "python",
   "name": "idmp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "090c59ccfcd44fc49b0d6b24987f0824c01aed8629dd5577dc9aff8c59f6d2bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
