{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib as mpl\n",
    "import tikzplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rich.progress import track\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import RocCurveDisplay, precision_recall_fscore_support, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import MaxAbsScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_files(root_path):\n",
    "    \"\"\"\n",
    "    Parse files and use pd.json_normalize to flatten the json.\n",
    "    \"\"\"\n",
    "    files = root_path.joinpath(\"train-output-all-features\").glob(\"**/*.json\")\n",
    "    results = pd.DataFrame()\n",
    "    files = list(files)\n",
    "\n",
    "    for file in track(\n",
    "        files,\n",
    "        description=\"Loading hyperparameters and \"\n",
    "        \"performance data from file to DataFrame\",\n",
    "    ):\n",
    "        with open(file) as f:\n",
    "            data = json.load(f)\n",
    "            original_target = root_path.joinpath(data[\"original_target\"])\n",
    "            original_dict = json.loads(Path(original_target).read_text())\n",
    "\n",
    "            f1_scores = pd.json_normalize(data, \"f1_scores\")\n",
    "            normalized_data = pd.json_normalize(original_dict)\n",
    "            normalized_data = pd.concat(\n",
    "                [normalized_data] * len(f1_scores), ignore_index=True\n",
    "            )\n",
    "            normalized_data[\"metrics.f1_score\"] = f1_scores\n",
    "            normalized_data[\"paths.model_path\"] = data[\"model_path\"]\n",
    "            results = pd.concat([results, normalized_data])\n",
    "\n",
    "    results = results.set_index(\n",
    "        [\"percentage\", \"model\", \"use_gradient\", \"use_ewma\"]\n",
    "    )\n",
    "    results.columns = pd.MultiIndex.from_arrays(\n",
    "        zip(*results.columns.str.split(\".\", expand=True))\n",
    "    )\n",
    "    results = results.sort_index(axis=1)\n",
    "    results = results.drop(columns=['k_fold', 'preprocessing'])\n",
    "    results = results.droplevel(0, axis=1)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "data_flattened = parse_files(Path(\"./analysis/\"))\n",
    "data_flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any runs that included use_ewma or use_gradient\n",
    "df = data_flattened.loc[(slice(None), slice(None), False, False), :]\n",
    "df.index = df.index.droplevel([2, 3])\n",
    "df = df.sort_values(by=['percentage', 'model'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_or_list_to_tuple(x):\n",
    "    if isinstance(x, list):\n",
    "        return tuple(x)\n",
    "    return x\n",
    "\n",
    "df[\"hidden_layer_sizes\"] = df['hidden_layer_sizes'].apply(float_or_list_to_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = df.groupby(['percentage', 'model', 'C', 'hidden_layer_sizes', 'kernel', 'learning_rate', 'max_depth', 'n_estimators', 'alpha'], dropna=False)\n",
    "summary = group['f1_score'].agg(['mean', 'std'])\n",
    "\n",
    "idx = summary.groupby(['percentage', 'model']).idxmax()\n",
    "\n",
    "best_hyperparams = summary.loc[idx['mean'], :]\n",
    "best_per_percentage = best_hyperparams.groupby('percentage')['mean'].idxmax()\n",
    "selected_percentages = best_hyperparams[best_hyperparams.index.get_level_values('percentage').isin([1, 5, 10, 15, 20]) & best_hyperparams.index.get_level_values('model').isin(['RF', 'ET', 'DT'])]\n",
    "models_to_test = selected_percentages.droplevel(['C', 'hidden_layer_sizes', 'kernel', 'learning_rate', 'alpha'])\n",
    "models_to_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "classifiers = {}\n",
    "for percentage, model, max_depth, n_estimators in models_to_test.index:\n",
    "    if np.isnan(max_depth):\n",
    "        max_depth = None\n",
    "    else:\n",
    "        max_depth = int(max_depth)\n",
    "    if model != 'DT':\n",
    "        n_estimators = int(n_estimators)\n",
    "    if model == 'ET':\n",
    "        classifiers[(percentage, model)] = ExtraTreesClassifier(max_depth=max_depth, n_estimators=n_estimators, random_state=random_state, class_weight='balanced')\n",
    "    elif model == 'RF':\n",
    "        classifiers[(percentage, model)] = RandomForestClassifier(max_depth=max_depth, n_estimators=n_estimators, random_state=random_state, class_weight='balanced_subsample')\n",
    "    elif model == 'DT':\n",
    "        classifiers[(percentage, model)] = DecisionTreeClassifier(max_depth=max_depth, random_state=random_state, class_weight='balanced')\n",
    "    classifiers[(percentage, 'Dum')] = DummyClassifier(strategy='stratified', random_state=random_state)\n",
    "classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def preprocess(dataframe, scaler=None):\n",
    "    result, mzn = dataframe.drop(columns=['mzn', 'dzn'], axis=1), dataframe['mzn']\n",
    "\n",
    "    # Drop any columns that contain the text 'ewma' or 'gradient'\n",
    "    result = result.drop(columns=result.columns[result.columns.str.contains('ewma|gradient')], axis=1)\n",
    "\n",
    "    \n",
    "    # result = result.drop(result.columns[result.nunique() == 1], axis=1)\n",
    "\n",
    "    if scaler is None:\n",
    "        result = result.drop(result.columns[result.nunique() == 1], axis=1)\n",
    "        scaler = MaxAbsScaler().fit(result)\n",
    "    else:\n",
    "        # Drop constant columns except those in scaler.feature_names_in_\n",
    "        constant_columns = result.columns[result.nunique() == 1]\n",
    "        features_in = scaler.feature_names_in_\n",
    "        columns_to_drop = constant_columns.difference(features_in)\n",
    "        result = result.drop(columns=columns_to_drop, axis=1)\n",
    "        \n",
    "        \n",
    "    result = pd.DataFrame(scaler.transform(result), columns=result.columns, index=result.index)\n",
    "\n",
    "    result['mzn'] = mzn\n",
    "\n",
    "    return result, scaler\n",
    "\n",
    "scalers = {}\n",
    "\n",
    "train_pkl = Path('./analysis/rerun_with_all_features_train.pkl')\n",
    "\n",
    "with open(train_pkl, 'rb') as f:\n",
    "    train_features_at_percentage = pickle.load(f)\n",
    "\n",
    "for percentage, model in classifiers:\n",
    "    print(f\"Training {model} at {percentage}%\")\n",
    "    df, scaler = preprocess(train_features_at_percentage[percentage])\n",
    "    scalers[(percentage, model)] = scaler\n",
    "    df = df.drop(columns=['mzn'], axis=1)\n",
    "    any_nan = df.isna().any().any()\n",
    "    \n",
    "    if any_nan:\n",
    "        print(\"The NaN values are in the following columns:\")\n",
    "        print(df.columns[df.isna().any()])\n",
    "    else:\n",
    "        train_X, train_y = df.drop(columns=['solved_within_time_limit']), df['solved_within_time_limit']\n",
    "        print(train_y.value_counts(normalize=False))\n",
    "        classifiers[(percentage, model)].fit(train_X, train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_features = pd.DataFrame({'feature': preprocess(train_features_at_percentage[1])[0].drop(['mzn', 'solved_within_time_limit'], axis=1).columns})\n",
    "describe_features['description'] = [\n",
    "    \"Number of conflicts\",\n",
    "    \"Number of decisions made/nodes\",\n",
    "    \"Number of search iterations completed\",\n",
    "    \"??? (number of assigned nodes or variables)\",\n",
    "    \"Number of variables\",\n",
    "    \"Number of backjumps\",\n",
    "    \"Number of solutions found\",\n",
    "    \"Total time spent\",\n",
    "    \"Time spent searching\",\n",
    "    \"Number of integer variables\",\n",
    "    \"Number of propagations\",\n",
    "    \"Number of SAT propagations\",\n",
    "    \"Number of propagators\",\n",
    "    \"Number of boolean variables\",\n",
    "    \"Number of learnt clauses\",\n",
    "    \"Number of binary clauses\",\n",
    "    \"Number of ternary clauses\",\n",
    "    \"Number of clauses longer than 3\",\n",
    "    \"The maximum decision level reached\",\n",
    "    \"Current decision level of the engine\",\n",
    "    \"Size of the tree of the decision level\",\n",
    "    \"Amount of memory used by clauses\",\n",
    "    \"Amount of memory used by propagators\",\n",
    "    \"Ratio of failures/conflicts to unassigned variables\",\n",
    "    \"Ratio of visited nodes to open nodes\",\n",
    "    \"Fraction of variables that are boolean\",\n",
    "    \"Ratio of propagations to variables\",\n",
    "    \"Fraction of clauses that are longer than 3\",\n",
    "    \"How often the engine backtracks (backjumps / total search time)\",\n",
    "]\n",
    "describe_features['translated'] = [\n",
    "    False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, False, True\n",
    "]\n",
    "describe_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_features.columns.str.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pickle = Path('./analysis/rerun_with_all_features_test.pkl')\n",
    "\n",
    "with open(test_pickle, 'rb') as f:\n",
    "    test_features_at_percentage = pickle.load(f)\n",
    "\n",
    "# Create a figure and axes with 4 subplots\n",
    "fig, ax = plt.subplots(2, 3, figsize=(6, 5), sharex=True, sharey=True)\n",
    "\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.3)\n",
    "ax = ax.flatten()\n",
    "scores = []\n",
    "\n",
    "for i, percentage in enumerate([1, 5, 10, 15, 20]):\n",
    "    for model in ['ET', 'RF', 'Dum']:\n",
    "        scaler = scalers[(percentage, model)]\n",
    "        df, _ = preprocess(test_features_at_percentage[percentage], scaler)\n",
    "        df = df.drop(columns=['mzn'], axis=1)\n",
    "        \n",
    "        test_X, test_y = df.drop(columns=['solved_within_time_limit']), df['solved_within_time_limit']\n",
    "        \n",
    "        y_score = classifiers[(percentage, model)].predict_proba(test_X)[:, 1]\n",
    "        if model != 'Dum':\n",
    "            display = RocCurveDisplay.from_predictions(test_y, y_score, ax=ax[i], name=model)\n",
    "            line = display.line_\n",
    "            display.line_.set_label(line.get_label().replace('AUC = ', ''))\n",
    "            ax[i].legend()\n",
    "            ax[i].set_xlabel('')\n",
    "            ax[i].set_ylabel('')\n",
    "\n",
    "        prec, rec, score, support = precision_recall_fscore_support(test_y, classifiers[(percentage, model)].predict(test_X), average='binary', pos_label=False)\n",
    "        accuracy = accuracy_score(test_y, classifiers[(percentage, model)].predict(test_X))\n",
    "        balanced_accuracy = balanced_accuracy_score(test_y, classifiers[(percentage, model)].predict(test_X))\n",
    "        # print(f\"For {model} at {percentage}%, Precision: {prec:.2f}, Recall: {rec:.2f}, F1: {score:.2f}\")\n",
    "        \n",
    "        scores.append([percentage, model, accuracy, balanced_accuracy, prec, rec, score])\n",
    "\n",
    "    ax[i].set_title(f\"{percentage}%\", fontsize=10)\n",
    "    ax[i].plot([0, 1], [0, 1], linestyle='--', lw=1, color='r', alpha=.2)\n",
    "            \n",
    "\n",
    "fig.supxlabel('False Positive Rate', fontsize=20, y=-0.05, x=0.54)\n",
    "fig.supylabel('True Positive Rate', fontsize=20, x=-0.01, y=0.4)\n",
    "ax[5].set_axis_off()\n",
    "fig.tight_layout()\n",
    "\n",
    "tikzplotlib.clean_figure(fig)\n",
    "tikzplotlib.save(\"roc_curves.tex\", axis_width=\"0.36\\\\textwidth\", axis_height=\"0.36\\\\textwidth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = pd.DataFrame(scores, columns=['Percentage', 'Model', 'Accuracy', 'Balanced Accuracy', 'Precision', 'Recall', 'F1 score'])\n",
    "score_df = score_df.set_index(['Percentage', 'Model']).unstack()\n",
    "score_df.index = score_df.index.astype(str)+\"%\"\n",
    "score_df.round(2).style.highlight_max(axis=1, props=\"font-weight:bold;\", subset=pd.IndexSlice[:, ['F1 score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(score_df.drop(columns=[\"Accuracy\"]).round(2)\n",
    "    .style.format_index(escape='latex')\n",
    "    .format(precision=2)\n",
    "    .highlight_max(axis=1, props=\"textbf:--rwrap;\", subset=pd.IndexSlice[:, ['F1 score']])\n",
    "    .to_latex(\n",
    "        \"performance-table.tex\",\n",
    "        hrules=True,\n",
    "        multicol_align='c',\n",
    "        caption=\"Performance measures for each model at varying \\% of TL.\",\n",
    "        label=\"tab:performance-measures\",\n",
    "        position_float=\"centering\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(100, 15))\n",
    "plot_tree(classifiers[(5, 'DT')], max_depth=6, feature_names=df.columns, class_names=['unsolved', 'solved'], filled=True, fontsize=8, proportion=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_importances(percentage, model, classifiers, test_X, test_y):\n",
    "    model = classifiers[(percentage, model)]\n",
    "    result = permutation_importance(model, test_X, test_y, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "    # model = classifiers[(percentage, model)]\n",
    "    # importance = model.feature_importances_\n",
    "    # std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\n",
    "    # return pd.DataFrame({'mean': importance, 'std': std}, index=model.feature_names_in_)\n",
    "    return pd.DataFrame({'mean': result.importances_mean, 'std': result.importances_std}, index=model.feature_names_in_)\n",
    "\n",
    "fig, ax = plt.subplots(5, 1, figsize=(10, 20), sharex=False, sharey=False)\n",
    "fig.subplots_adjust(hspace=1, wspace=0.3)\n",
    "ax = ax.flatten()\n",
    "\n",
    "for i, percentage in enumerate([1, 5, 10, 15, 20]):\n",
    "    # percentage = 5\n",
    "    df, _ = preprocess(test_features_at_percentage[percentage], scaler)\n",
    "    df = df.drop(columns=['mzn'], axis=1)\n",
    "    test_X, test_y = df.drop(columns=['solved_within_time_limit']), df['solved_within_time_limit']\n",
    "    importance = get_importances(percentage, 'RF', classifiers, test_X, test_y).sort_values(by='mean', ascending=False)\n",
    "    ax[i] = importance['mean'].plot.bar(yerr=importance['std'], capsize=2, ax=ax[i], title=f\"{percentage}%\", rot=90)\n",
    "\n",
    "# plt.barh(m.index, m['importance'], xerr=s['std'], capsize=2)\n",
    "# rf_importances = pd.DataFrame([get_importances(x, c, classifiers) for x in [5, 10, 15, 20] for c in ['RF']], index=[5, 10, 15, 20])\n",
    "# et_importances = pd.DataFrame([get_importances(x, c, classifiers) for x in [5, 10, 15, 20] for c in ['ET']], index=[5, 10, 15, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = get_importances(percentage, 'ET', classifiers, test_X, test_y).sort_values(by='mean', ascending=False)\n",
    "importance['mean'].plot.bar(yerr=importance['std'], capsize=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = train_features_at_percentage #{i: pd.concat([train_features_at_percentage[i], test_features_at_percentage[i]]) for i in range(1, len(train_features_at_percentage) + 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.stats import spearmanr\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def get_rankings_for_percentages(percentage, depth, model='RF'):\n",
    "    df, _ = preprocess(all_features[percentage], scaler)\n",
    "    df = df.drop(columns=['mzn'], axis=1)\n",
    "    X, y = df.drop(columns=['solved_within_time_limit']), df['solved_within_time_limit']\n",
    "\n",
    "    # fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
    "    corr = spearmanr(X).correlation\n",
    "\n",
    "    # Ensure the correlation matrix is symmetric\n",
    "    corr = (corr + corr.T) / 2\n",
    "    np.fill_diagonal(corr, 1)\n",
    "\n",
    "    # We convert the correlation matrix to a distance matrix before performing\n",
    "    # hierarchical clustering using Ward's linkage.\n",
    "    distance_matrix = 1 - np.abs(corr)\n",
    "    dist_linkage = hierarchy.ward(squareform(distance_matrix))\n",
    "    # dendro = hierarchy.dendrogram(\n",
    "    #     dist_linkage, labels=X.columns.to_list(), ax=ax1, leaf_rotation=90\n",
    "    # )\n",
    "    # dendro_idx = np.arange(0, len(dendro[\"ivl\"]))\n",
    "\n",
    "    # ax2.imshow(corr[dendro[\"leaves\"], :][:, dendro[\"leaves\"]])\n",
    "    # ax2.set_xticks(dendro_idx)\n",
    "    # ax2.set_yticks(dendro_idx)\n",
    "    # ax2.set_xticklabels(dendro[\"ivl\"], rotation=\"vertical\")\n",
    "    # ax2.set_yticklabels(dendro[\"ivl\"])\n",
    "    # _ = fig.tight_layout()\n",
    "\n",
    "    df, _ = preprocess(train_features_at_percentage[percentage], scaler)\n",
    "    df = df.drop(columns=['mzn'], axis=1)\n",
    "    X_train, y_train = df.drop(columns=['solved_within_time_limit']), df['solved_within_time_limit']\n",
    "\n",
    "    df, _ = preprocess(test_features_at_percentage[percentage], scaler)\n",
    "    df = df.drop(columns=['mzn'], axis=1)\n",
    "    X_test, y_test = df.drop(columns=['solved_within_time_limit']), df['solved_within_time_limit']\n",
    "\n",
    "    cluster_ids = hierarchy.fcluster(dist_linkage, depth, criterion=\"distance\")\n",
    "\n",
    "    cluster_id_to_feature_ids = defaultdict(list)\n",
    "    for idx, cluster_id in enumerate(cluster_ids):\n",
    "        cluster_id_to_feature_ids[cluster_id].append(idx)\n",
    "    selected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n",
    "    selected_features_names = X.columns[selected_features]\n",
    "\n",
    "    X_train_sel = X_train[selected_features_names]\n",
    "    X_test_sel = X_test[selected_features_names]\n",
    "\n",
    "    clf_sel = RandomForestClassifier(n_estimators=100, random_state=42) if model == 'RF' else ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "    clf_sel.fit(X_train_sel, y_train)\n",
    "    _, _, f1, _ = precision_recall_fscore_support(y_test, clf_sel.predict(X_test_sel), average='binary', pos_label=False)\n",
    "    # print(\n",
    "    #     \"Baseline f1 on test data with features removed:\"\n",
    "    #     # f\" {clf_sel.score(X_test_sel, y_test):.2}\"\n",
    "    #     f\" {f1:.2}\"\n",
    "    # )\n",
    "\n",
    "    def plot_permutation_importance(clf, X, y, ax, selected_features, cluster_id_to_feature_ids, all_columns):\n",
    "        result = permutation_importance(clf, X, y, n_repeats=10, random_state=42, n_jobs=2)\n",
    "        perm_sorted_idx = result.importances_mean.argsort()\n",
    "        labels = []\n",
    "        for i in np.array(selected_features)[perm_sorted_idx]:\n",
    "            for k, v in cluster_id_to_feature_ids.items():\n",
    "                if i in v:\n",
    "                    main_feature = all_columns[i]\n",
    "                    all_features = all_columns[v].difference([main_feature])\n",
    "                    labels.append(str(main_feature) + \" \" + str(list(all_features)).replace('[', '(').replace(']', ')'))\n",
    "\n",
    "        ax.boxplot(\n",
    "            result.importances[perm_sorted_idx].T,\n",
    "            vert=False,\n",
    "            labels=X.columns[perm_sorted_idx],\n",
    "            # labels=labels,\n",
    "        )\n",
    "        ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "        return ax, list(reversed(labels))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(17, 6))\n",
    "    _, labels = plot_permutation_importance(clf_sel, X_test_sel, y_test, ax, selected_features, cluster_id_to_feature_ids, X.columns)\n",
    "    # ax.set_title(\"Permutation Importances on selected subset of features\\n(test set)\")\n",
    "    # ax.set_xlabel(\"Decrease in accuracy score\")\n",
    "    # ax.figure.tight_layout()\n",
    "    # plt.show()\n",
    "    return labels\n",
    "\n",
    "importance_df = []\n",
    "for perc in [1, 5, 10, 15, 20]:\n",
    "    print(f\"Percentage: {perc}\")\n",
    "    labels_with_groups_rf = get_rankings_for_percentages(perc, 1.1, model='RF')\n",
    "    just_labels_rf = [x.split()[0] for x in labels_with_groups_rf[:5]]  # Get the first 5 labels\n",
    "    labels_with_groups_et = get_rankings_for_percentages(perc, 1.1, model='ET')\n",
    "    just_labels_et = [x.split()[0] for x in labels_with_groups_et[:5]]  # Get the first 5 labels\n",
    "    labels = just_labels_rf + just_labels_et\n",
    "    importance_df.append(labels)\n",
    "    print(labels)\n",
    "importance_df = pd.DataFrame(importance_df, index=['1%', '5%', '10%', '15%', '20%'], columns=pd.MultiIndex.from_product([['RF', 'ET'], range(1, 6)]))\n",
    "importance_df.index.name = 'Percentage'\n",
    "importance_df.columns.name\n",
    "# importance_df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df.columns.names = ['Model', 'Rank']\n",
    "(importance_df['RF']\n",
    "        .replace('intVars', 'int_vars')\n",
    "        .replace('bin', 'bin_clauses')\n",
    "        .style\n",
    "        .format_index(escape='latex')\n",
    "        .format(\"\\\\texttt{{{}}}\", escape='latex')\n",
    "        .to_latex(\"importance-table.tex\",\n",
    "                hrules=True,\n",
    "                multicol_align='c',\n",
    "                caption=\"Permutation importance rankings for RF at varying \\% of TL. Rank 1 is the most important feature, rank 5 is less important.\",\n",
    "                label=\"tab:importance-measures\",\n",
    "                position_float=\"centering\",\n",
    "        )\n",
    ")\n",
    "importance_df['RF'].replace('intVars', 'int_vars').replace('bin', 'bin_clauses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\", \".join([f\"\\\\texttt{{{x}}}\" for x in {l.split()[0]: l.replace(')', '').replace('(', '').replace(\"'\", \"\").replace(\",\", \"\").replace('_', '\\_').split()[1:] for l in labels_with_groups_rf}['bin']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,7))\n",
    "plot_permutation_importance(classifiers[(percentage, 'DT')], X_test, y_test, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total = np.array([train_features_at_percentage[i].shape[0] for i in range(1, len(train_features_at_percentage) + 1)])\n",
    "n_solved = np.array([train_features_at_percentage[i]['solved_within_time_limit'].sum() for i in range(1, len(train_features_at_percentage) + 1)])\n",
    "n_not_solved = np.array([n_total[i] - n_solved[i] for i in range(len(n_total))])\n",
    "percentage_solved = np.array([n_solved[i] / n_total[i] for i in range(len(n_total))]) * 100\n",
    "\n",
    "x = np.arange(1, 20.5, 0.5)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 3))\n",
    "stacks = ax.stackplot(x, n_not_solved, n_solved, labels=['Not solved', 'Solved'])\n",
    "ax.set_title(\"Number of instances at varying % of TL\")\n",
    "ax.set_xlabel(\"% of TL\")\n",
    "ax.set_ylabel(\"Total Number of instances\")\n",
    "ax.set_ylim([0, 1400])\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(0.98, 0.98))\n",
    "\n",
    "# fig.legend(loc='upper right', bbox_to_anchor=(0.88, 0.88))\n",
    "# fig.tight_layout()\n",
    "# plt.savefig(\"class_balance_over_time.pgf\", backend=\"pgf\")\n",
    "# tikzplotlib.clean_figure()\n",
    "tikzplotlib.save(\"class_balance_over_time.tex\", axis_width=\"0.9\\\\textwidth\", axis_height=\"0.4\\\\textwidth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import enum\n",
    "\n",
    "class ProblemType(enum.Enum):\n",
    "    UNKNOWN = 0\n",
    "    SAT = 1\n",
    "    OPT = 2\n",
    "\n",
    "fzn_files = list(glob.glob(f\"analysis/problems_compiled/*.fzn\"))\n",
    "summary = []\n",
    "\n",
    "\n",
    "for fzn_file in track(fzn_files, description=\"Counting SAT/OPT\"):\n",
    "    with open(fzn_file, \"r\") as f:\n",
    "        contents = f.read()\n",
    "        problem_type = None\n",
    "        if \"satisfy\" in contents:\n",
    "            problem_type = ProblemType.SAT\n",
    "        elif \"minimize\" in contents or \"maximize\" in contents:\n",
    "            problem_type = ProblemType.OPT\n",
    "        else:\n",
    "            problem_type = ProblemType.UNKNOWN\n",
    "        \n",
    "        summary.append({\n",
    "            \"Problem\": fzn_file,\n",
    "            \"Problem Type\": problem_type\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary, dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_str = r'analysis/problems_compiled/.+MZN-(.+)-DZN.+\\.fzn'\n",
    "all_mzn = summary_df['Problem'].str.extract(regex_str, expand=False) + '.mzn'\n",
    "summary_df['mzn'] = all_mzn\n",
    "regex_str = r'analysis/problems_compiled/.+DZN-(.+)\\.fzn'\n",
    "all_dzn = summary_df['Problem'].str.extract(regex_str, expand=False) + '.dzn'\n",
    "summary_df['dzn'] = all_dzn\n",
    "\n",
    "half_percent_feautures = all_features[1]\n",
    "\n",
    "summary_df['Past Half Percent'] = summary_df[['mzn', 'dzn']].apply(tuple, axis=1).isin(half_percent_feautures[['mzn', 'dzn']].apply(tuple, axis=1))\n",
    "summary_past = summary_df[summary_df['Past Half Percent']]\n",
    "summary_past\n",
    "\n",
    "grouped_summary_df = summary_df.groupby(['Problem Type', 'Past Half Percent']).count().drop(columns=['mzn', 'dzn']).unstack()\n",
    "grouped_summary_df = grouped_summary_df.rename(columns={'Problem': 'Count'}).rename(index={str(ProblemType.SAT): 'SAT', str(ProblemType.OPT): 'OPT', ProblemType.UNKNOWN: 'Unknown'}).droplevel(0, axis=1).rename(columns={True: \"$\\ge$0.5TL\\%\", False: \"<0.5TL\\%\"})\n",
    "grouped_summary_df['Total'] = grouped_summary_df.sum(axis=1)\n",
    "cols = grouped_summary_df.columns\n",
    "grouped_summary_df[[cols[2], cols[0], cols[1]]].rename_axis(None, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
